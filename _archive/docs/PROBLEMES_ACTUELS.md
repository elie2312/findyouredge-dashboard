# âš ï¸ ProblÃ¨mes Actuels du Dashboard

## ğŸ¯ Ã‰tat Actuel : 85% Fonctionnel

Le dashboard est **visuellement complet** mais il manque l'**intÃ©gration backend complÃ¨te** pour lancer les backtests.

---

## âœ… Ce qui Fonctionne (85%)

### Frontend
- âœ… Page d'accueil avec statistiques
- âœ… Page Run avec sÃ©lection de stratÃ©gies
- âœ… Page Results avec charts
- âœ… Page Compare
- âœ… Navigation complÃ¨te
- âœ… Dark mode
- âœ… Animations
- âœ… Design moderne
- âœ… Boutons "Voir rÃ©sultats" sur la home

### Backend
- âœ… API FastAPI dÃ©marrÃ©e
- âœ… Liste des stratÃ©gies (7 stratÃ©gies dÃ©tectÃ©es)
- âœ… Liste des runs existants
- âœ… Connexion aux adapters
- âœ… CORS configurÃ©

---

## âŒ Ce qui Ne Fonctionne Pas (15%)

### 1. Lancement de Backtests

**ProblÃ¨me :** Quand vous cliquez sur "Lancer un Backtest", Ã§a crÃ©e un Run ID mais ne lance pas vraiment le script Python.

**Fichier concernÃ© :** `backend/routers/runs.py` lignes 46-72

**Code actuel (placeholder) :**
```python
@router.post("", response_model=RunResponse)
def create_run(request: RunRequest, background_tasks: BackgroundTasks):
    runner = get_runner()
    run_id = f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:6]}"
    
    # TODO: ImplÃ©menter le lancement async
    # Pour l'instant, on simule
    # background_tasks.add_task(runner.run_async, request.strategy_id, request.parameters)
    
    return RunResponse(
        run_id=run_id,
        status="running",
        message=f"Backtest {request.strategy_id} dÃ©marrÃ©"
    )
```

**Ce qu'il faudrait faire :**
```python
# Lancer vraiment le backtest
strategy = strategiesApi.get(request.strategy_id)
result = runner.run(
    script_path=strategy.script_path,
    parameters=request.parameters
)
```

---

### 2. Statut des Runs

**ProblÃ¨me :** Le statut retourne 404 car le run n'existe pas vraiment.

**Logs backend :**
```
POST /api/runs HTTP/1.1" 200 OK
GET /api/runs/{id}/status HTTP/1.1" 404 Not Found
```

**Raison :** Le run n'est jamais sauvegardÃ© ni exÃ©cutÃ©.

**Ce qu'il faudrait :**
- Sauvegarder le run dans une DB ou fichier JSON
- Lancer le script Python en arriÃ¨re-plan
- Mettre Ã  jour le statut (running â†’ completed/failed)

---

### 3. RÃ©sultats

**ProblÃ¨me :** Pas de rÃ©sultats car les backtests ne sont pas exÃ©cutÃ©s.

**Ce qu'il faudrait :**
- ExÃ©cuter le backtest
- Parser les rÃ©sultats (CSV, JSON)
- Les renvoyer via `/api/runs/{id}/results`

---

### 4. Comparaison

**ProblÃ¨me :** Pas de donnÃ©es dans la table de comparaison.

**Message visible :** 
> ğŸ’¡ **Note:** Pour afficher les mÃ©triques complÃ¨tes, l'endpoint `/api/runs/compare` doit Ãªtre implÃ©mentÃ© dans le backend.

**Ce qu'il faudrait :**
- CrÃ©er endpoint `POST /api/runs/compare`
- Accepter une liste de run_ids
- Retourner les mÃ©triques comparÃ©es

---

## ğŸ”§ Solutions

### Solution 1 : Test avec les Runs Existants (Rapide)

Si vous avez dÃ©jÃ  des runs terminÃ©s dans `NQ/integration_poc/runs/`, vous pouvez:

1. VÃ©rifier qu'ils sont bien dÃ©tectÃ©s :
```bash
cd backend
python
>>> from run_backtest import create_runner
>>> runner = create_runner("../NQ")
>>> runs = runner.list_runs()
>>> for r in runs:
...     print(r.run_id, r.status)
```

2. Si des runs sont "completed", cliquez sur "ğŸ“Š Voir rÃ©sultats" dans la home page

---

### Solution 2 : ImplÃ©menter le Lancement (30-60 min)

**Fichier Ã  modifier :** `backend/routers/runs.py`

**Ã‰tapes :**
1. RÃ©cupÃ©rer la stratÃ©gie sÃ©lectionnÃ©e
2. Utiliser `runner.run()` pour lancer le script
3. Sauvegarder le run_id dans un fichier JSON
4. Mettre Ã  jour le statut aprÃ¨s exÃ©cution

**Code Ã  implÃ©menter :**
```python
import subprocess
import json
from pathlib import Path

RUNS_DB = Path(__file__).parent.parent / "runs_db.json"

def save_run(run_id, status, message):
    if RUNS_DB.exists():
        data = json.loads(RUNS_DB.read_text())
    else:
        data = {"runs": []}
    
    data["runs"].append({
        "run_id": run_id,
        "status": status,
        "message": message,
        "created_at": datetime.now().isoformat()
    })
    
    RUNS_DB.write_text(json.dumps(data, indent=2))

@router.post("", response_model=RunResponse)
def create_run(request: RunRequest, background_tasks: BackgroundTasks):
    runner = get_runner()
    run_id = f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:6]}"
    
    # RÃ©cupÃ©rer la stratÃ©gie
    strategies = get_available_strategies(str(NQ_BASE_PATH))
    strategy = next((s for s in strategies if s['name'].lower().replace(' ', '_') == request.strategy_id), None)
    
    if not strategy:
        raise HTTPException(404, "StratÃ©gie non trouvÃ©e")
    
    # Sauvegarder le run
    save_run(run_id, "running", f"Backtest {strategy['name']} dÃ©marrÃ©")
    
    # Lancer en arriÃ¨re-plan
    def run_backtest():
        try:
            # Lancer le script Python
            result = subprocess.run(
                ["python", strategy['script_path']],
                capture_output=True,
                text=True,
                timeout=300  # 5 minutes max
            )
            
            if result.returncode == 0:
                save_run(run_id, "completed", "Backtest terminÃ© avec succÃ¨s")
            else:
                save_run(run_id, "failed", f"Erreur: {result.stderr}")
        except Exception as e:
            save_run(run_id, "failed", str(e))
    
    background_tasks.add_task(run_backtest)
    
    return RunResponse(
        run_id=run_id,
        status="running",
        message=f"Backtest {strategy['name']} dÃ©marrÃ©",
        started_at=datetime.now().isoformat()
    )
```

---

### Solution 3 : Utiliser le SystÃ¨me Existant (5 min)

**Si vous avez dÃ©jÃ  un systÃ¨me pour lancer les backtests** (scripts, CLI, etc.) :

1. Lancez vos backtests comme d'habitude
2. Les rÃ©sultats apparaÃ®tront automatiquement dans le dashboard
3. Cliquez sur "ğŸ“Š Voir rÃ©sultats" pour les visualiser

---

## ğŸ“Š RÃ©sumÃ©

| FonctionnalitÃ© | Ã‰tat | Action |
|----------------|------|--------|
| Design Frontend | âœ… 100% | Rien |
| Navigation | âœ… 100% | Rien |
| API StratÃ©gies | âœ… 100% | Rien |
| API Liste Runs | âœ… 100% | Rien |
| **Lancement Backtests** | âŒ 0% | **ImplÃ©menter** |
| **Statut Runs** | âŒ 0% | **ImplÃ©menter** |
| **RÃ©sultats** | âš ï¸ 50% | Fonctionne avec runs existants |
| **Comparaison** | âš ï¸ 30% | Endpoint Ã  crÃ©er |

---

## ğŸ¯ PrioritÃ©s

### PrioritÃ© 1 : Tester avec Runs Existants
Si vous avez des runs terminÃ©s, testez la visualisation des rÃ©sultats.

### PrioritÃ© 2 : ImplÃ©menter le Lancement
Modifier `backend/routers/runs.py` pour lancer vraiment les backtests.

### PrioritÃ© 3 : Endpoint Comparaison
CrÃ©er `POST /api/runs/compare` pour comparer plusieurs runs.

---

## ğŸ’¡ Workaround Temporaire

**Pour tester le dashboard sans implÃ©menter le lancement :**

1. Lancez un backtest manuellement (comme avant)
2. Attendez qu'il se termine
3. RafraÃ®chissez le dashboard
4. Les rÃ©sultats apparaÃ®tront automatiquement
5. Cliquez sur "ğŸ“Š Voir rÃ©sultats"

---

**CrÃ©Ã© le :** 09/10/2025  
**Status Dashboard :** 85% fonctionnel (design complet, backend partiel)
